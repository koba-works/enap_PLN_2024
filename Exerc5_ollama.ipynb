{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN4+ImuCRjRceQqzLPk995G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koba-works/enap_PLN_2024/blob/main/Exerc5_ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ollama in Google Colab\n",
        "\n",
        "This repository provides instructions and code snippets for using [Ollama](https://github.com/ollama/ollama) in Google Colab notebooks.\n",
        "\n",
        "## Installation\n",
        "\n",
        "To install Ollama in your Colab environment, follow these steps:\n",
        "\n",
        "1. Run the following command in a code cell to install the required dependencies:\n"
      ],
      "metadata": {
        "id": "KLd0zMNNeSP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxm0EmuddmP0"
      },
      "outputs": [],
      "source": [
        "! sudo apt-get install -y pciutils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Run the installation script provided by Ollama:"
      ],
      "metadata": {
        "id": "q77ySC0eeYUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "4YyfVpSqeaaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries and define the Ollama function:"
      ],
      "metadata": {
        "id": "ltD8xDlnedlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    import os\n",
        "    import threading\n",
        "    import subprocess\n",
        "    import requests\n",
        "    import json\n",
        "\n",
        "    def ollama():\n",
        "        os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "        os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "        subprocess.Popen([\"ollama\", \"serve\"])"
      ],
      "metadata": {
        "id": "N0xTm5woejUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage\n",
        "\n",
        "Once Ollama is installed, you can use it in your Colab notebook as follows:\n",
        "\n",
        "1. Start the Ollama server by running the following code:"
      ],
      "metadata": {
        "id": "lVGcaSDJeoLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    ollama_thread = threading.Thread(target=ollama)\n",
        "    ollama_thread.start()"
      ],
      "metadata": {
        "id": "0uPs4XIseqx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Run the Ollama model of your choice. For example, to use the `mistral` model, execute:"
      ],
      "metadata": {
        "id": "ESLmARQtetUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ollama run mistral"
      ],
      "metadata": {
        "id": "QkBrqRymewi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After seeing this message `Send a message (/? for help)`, stop the execution and proceed to the next step."
      ],
      "metadata": {
        "id": "8KyvooYxe2fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Now you need to start the Ollama server again by running the following code:"
      ],
      "metadata": {
        "id": "IKzYteRde3dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    ollama_thread = threading.Thread(target=ollama)\n",
        "    ollama_thread.start()"
      ],
      "metadata": {
        "id": "6aadsoUYe6hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Now, you can interact with Ollama by sending prompts and receiving responses. Here's an example:"
      ],
      "metadata": {
        "id": "EreH1w1Ve5pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    prompt = \"\"\"\n",
        "    What is AI?\n",
        "    Can you explain in three paragraphs?\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "p4b3Xyk1e_MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Then, run the following code to receive the response based on your prompt. Here, `stream` is set to `False`, but you can also consider a streaming approach for continuous response printing:"
      ],
      "metadata": {
        "id": "ZJp7TgALfDJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    url = 'http://localhost:11434/api/chat'\n",
        "    payload = {\n",
        "        \"model\": \"mistral\",\n",
        "        \"temperature\": 0.6,\n",
        "        \"stream\": False,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI assistant!\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload)\n",
        "    message_str = response.content.decode('utf-8')\n",
        "    message_dict = json.loads(message_str)\n",
        "    print(message_dict['message']['content'])"
      ],
      "metadata": {
        "id": "FVs6yqaOfGI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will send the prompt to the Ollama model and print its response.\n",
        "\n",
        "## License\n",
        "\n",
        "This content is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
        "\n",
        "## Support Us\n",
        "\n",
        "If you find it helpful, consider supporting us in the following ways:\n",
        "\n",
        "- ‚≠ê Star this repository on [GitHub](https://github.com/AITwinMinds/Ollama-in-Google-Colab).\n",
        "  \n",
        "- üê¶ Follow us on X (Twitter): [@AITwinMinds](https://twitter.com/AITwinMinds)\n",
        "\n",
        "- üì£ Join our Telegram Channel: [AITwinMinds](https://t.me/AITwinMinds) for discussions and announcements.\n",
        "\n",
        "- üé• Subscribe to our YouTube Channel: [AITwinMinds](https://www.youtube.com/@AITwinMinds) for video tutorials and updates.\n",
        "\n",
        "- üì∏ Follow us on Instagram: [@AITwinMinds](https://www.instagram.com/AITwinMinds)\n",
        "\n",
        "Don't forget to share it with your friends!\n",
        "\n",
        "## Contact\n",
        "\n",
        "For any inquiries, please contact us at [AITwinMinds@gmail.com](mailto:AITwinMinds@gmail.com)."
      ],
      "metadata": {
        "id": "0l2qa36yfMAG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFFaxwnEfNlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}